# Multi-Provider LLM API Configuration Template
# Copy this file to .env and fill in your actual values

# =================================================================
# Default Provider Selection
# =================================================================
# Options: deepseek, openai, anthropic, local
DEFAULT_LLM_PROVIDER=deepseek

# =================================================================
# DeepSeek Configuration
# =================================================================
# Your DeepSeek API key (get from https://platform.deepseek.com/)
DEEPSEEK_API_KEY=your_actual_deepseek_api_key_here

# DeepSeek API base URL (usually no need to change)
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

# Model selection (auto-selected based on task complexity)
DEEPSEEK_CHAT_MODEL=deepseek-chat
DEEPSEEK_REASONER_MODEL=deepseek-reasoner

# =================================================================
# OpenAI Configuration
# =================================================================
# Your OpenAI API key (get from https://platform.openai.com/)
OPENAI_API_KEY=your_actual_openai_api_key_here

# OpenAI API base URL (usually no need to change)
OPENAI_BASE_URL=https://api.openai.com/v1

# =================================================================
# Anthropic Configuration
# =================================================================
# Your Anthropic API key (get from https://console.anthropic.com/)
ANTHROPIC_API_KEY=your_actual_anthropic_api_key_here

# Anthropic API base URL (usually no need to change)
ANTHROPIC_BASE_URL=https://api.anthropic.com/v1

# =================================================================
# Local Models (Ollama) Configuration
# =================================================================
# No API key required for local models
# Install Ollama from https://ollama.ai and run: ollama serve
LOCAL_BASE_URL=http://localhost:11434/v1
# Note: Make sure to pull models first: ollama pull llama3
